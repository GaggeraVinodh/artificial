{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDBNzanDGgWn"
   },
   "source": [
    "## Lesson 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "x7-IG3c0CUhG"
   },
   "outputs": [],
   "source": [
    "DATA_SET_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ja1sRWqmCeYe"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "\n",
    "def get_data(url, fn):\n",
    "  response = requests.get(url)\n",
    "  f = open(fn, 'wb')\n",
    "  f.write(response.content)\n",
    "  f.close()\n",
    "\n",
    "def extract_data(fn):\n",
    "  with zipfile.ZipFile(fn, 'r') as zf:\n",
    "    zf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TpjgNZRuDYjC"
   },
   "outputs": [],
   "source": [
    "DATA_SET_FILE_NAME = 'dataset.zip'\n",
    "\n",
    "get_data(DATA_SET_URL, DATA_SET_FILE_NAME)\n",
    "extract_data(DATA_SET_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GWKOAHvpDfTd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rename_data_folder(fn, new_name):\n",
    "  if os.path.exists(fn):\n",
    "    os.rename(fn, new_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "erqV1zQfzEH6"
   },
   "outputs": [],
   "source": [
    "DATA_SET_FOLDER = 'sentiment labelled sentences'\n",
    "\n",
    "rename_data_folder(DATA_SET_FOLDER, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kew_aOg7GJ0P"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label source\n",
       "0                           Wow... Loved this place.      1   yelp\n",
       "1                                 Crust is not good.      0   yelp\n",
       "2          Not tasty and the texture was just nasty.      0   yelp\n",
       "3  Stopped by during the late May bank holiday of...      1   yelp\n",
       "4  The selection on the menu was great and so wer...      1   yelp"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filepath_dict = {\n",
    "    'yelp': 'data/yelp_labelled.txt',\n",
    "    'amazon': 'data/amazon_cells_labelled.txt',\n",
    "    'imdb': 'data/imdb_labelled.txt'\n",
    "}\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for source, filepath in filepath_dict.items():\n",
    "  df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
    "  df['source'] = source\n",
    "  df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3Z2tNGjICb_"
   },
   "source": [
    "## Lesson 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0-owWx9VG1ch"
   },
   "outputs": [],
   "source": [
    "sentences = ['John likes ice cream', 'John hates chocolate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cwK6tSDgJzVf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'John': 0, 'likes': 5, 'ice': 4, 'cream': 2, 'hates': 3, 'chocolate': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "vectorizer.fit(sentences)\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ut3vMO-aKonx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 1],\n",
       "       [1, 1, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cpd9nJyEKaUU"
   },
   "source": [
    "## Lesson 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xePplChgLN4s"
   },
   "outputs": [],
   "source": [
    "df_yelp = df[df['source'] == 'yelp']\n",
    "\n",
    "sentences = df_yelp['sentence'].values\n",
    "labels = df_yelp['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ir6SRI4fUiOy"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5Gbtl60sW_vB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<750x1714 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7368 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences_train)\n",
    "\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "X_test = vectorizer.transform(sentences_test)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FvTjGa8aYeA5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "YmdSezq-bjcn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp: 0.7960\n",
      "amazon: 0.7960\n",
      "imdb: 0.7487\n"
     ]
    }
   ],
   "source": [
    "for source in df['source'].unique():\n",
    "  df_source = df[df['source'] == source]\n",
    "  sentences = df_source['sentence'].values\n",
    "  labels = df_source['label'].values\n",
    "\n",
    "  sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.25, random_state=1000)\n",
    "\n",
    "  vectorizer = CountVectorizer()\n",
    "  vectorizer.fit(sentences_train)\n",
    "  X_train = vectorizer.transform(sentences_train)\n",
    "  X_test = vectorizer.transform(sentences_test)\n",
    "\n",
    "  classifier = LogisticRegression()\n",
    "  classifier.fit(X_train, y_train)\n",
    "  score = classifier.score(X_test, y_test)\n",
    "\n",
    "  print(f'{source}: {score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bZhZil_IPl6"
   },
   "source": [
    "## Lesson 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hMvIQGH2K1KW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 1714)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp = df[df['source'] == 'yelp']\n",
    "\n",
    "sentences = df_yelp['sentence'].values\n",
    "labels = df_yelp['label'].values\n",
    "\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.25, random_state=1000)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences_train)\n",
    "\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "X_test = vectorizer.transform(sentences_test)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "I-6P68otdh5v"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0aa49a6dfed6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "H1myApldIzv_"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1e8abd429289>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(10, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0QiCvr9LH2j"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MROXYIlILN4m"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkuFRhe_jHDR"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4UmLLaamHtL"
   },
   "outputs": [],
   "source": [
    "_, train_accuracy = model.evaluate(X_train, y_train)\n",
    "_, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Training accuracy: {train_accuracy:.4f}')\n",
    "print(f'Testing accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCFB714qfEzz"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q034rvTPj1xY"
   },
   "outputs": [],
   "source": [
    "type(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxpFpo-Dj4G4"
   },
   "outputs": [],
   "source": [
    " len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuggRSThLRon"
   },
   "source": [
    "## Lesson 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBktcQb-nv29"
   },
   "outputs": [],
   "source": [
    "cities = ['London', 'Berlin', 'Berlin', 'New York', 'London']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "city_labels = encoder.fit_transform(cities)\n",
    "\n",
    "city_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAwpjiOVqX0Q"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "city_labels = city_labels.reshape((5, 1))\n",
    "one_hot_encoder.fit_transform(city_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kn48mmxJrHWb"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "print(sentences_train[2])\n",
    "print(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m03oIWHTtEg9"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=100)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=100)\n",
    "\n",
    "print(sentences_train[2])\n",
    "print(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqBHPF3UwhpK"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(\n",
    "    layers.Embedding(\n",
    "        input_dim=len(tokenizer.word_index) + 1, \n",
    "        output_dim=50, \n",
    "        input_length=100\n",
    "    )\n",
    ")\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKlL62E0NAuQ"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaOjP-wrOKdg"
   },
   "outputs": [],
   "source": [
    "_, train_accuracy = model.evaluate(X_train, y_train)\n",
    "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "_, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Testing Accuracy: {test_accuracy:.4f}')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpsZQxN2wx3Q"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(\n",
    "    layers.Embedding(\n",
    "        input_dim=len(tokenizer.word_index) + 1, \n",
    "        output_dim=50, \n",
    "        input_length=100\n",
    "    )\n",
    ")\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    validation_data=(X_test, y_test), \n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "_, train_accuracy = model.evaluate(X_train, y_train)\n",
    "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "_, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Testing Accuracy: {test_accuracy:.4f}')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjCC8mpmLbp2"
   },
   "source": [
    "## Lesson 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDEC43BsxHcO"
   },
   "outputs": [],
   "source": [
    "GLOVE_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "\n",
    "GLOVE_FILE_NAME = 'glove.zip'\n",
    "\n",
    "get_data(GLOVE_URL, GLOVE_FILE_NAME)\n",
    "extract_data(GLOVE_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKjQcQyW29EL"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "  vocab_size = len(word_index) + 1\n",
    "  embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "  with open(filepath) as f:\n",
    "    for line in f:\n",
    "      word, *vector = line.split()\n",
    "      if word in word_index:\n",
    "        idx = word_index[word]\n",
    "        embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "  return embedding_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUbQMSIj4Aiw"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = create_embedding_matrix('glove.6B.50d.txt', tokenizer.word_index, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lhToLf94Ko7"
   },
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lMxo_0G4xyv"
   },
   "outputs": [],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / (len(tokenizer.word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHoUUcul4--2"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=len(tokenizer.word_index) + 1, \n",
    "    output_dim=50, \n",
    "    input_length=100,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYgp4CO050Eu"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=50, \n",
    "    validation_data=(X_test, y_test), \n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "_, train_accuracy = model.evaluate(X_train, y_train)\n",
    "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "_, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Testing Accuracy: {test_accuracy:.4f}')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVTyG3Ru6DOK"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=len(tokenizer.word_index) + 1, \n",
    "    output_dim=50, \n",
    "    input_length=100,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=True))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaEuVGrk6jwb"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=50, \n",
    "    validation_data=(X_test, y_test), \n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "_, train_accuracy = model.evaluate(X_train, y_train)\n",
    "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "_, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Testing Accuracy: {test_accuracy:.4f}')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKaJ3wQZLpRJ"
   },
   "source": [
    "## Lesson 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCUCkAaH6mMY"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=len(tokenizer.word_index) + 1, \n",
    "    output_dim=100, \n",
    "    input_length=100))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBtd6mro0-OV"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ylXMRFIcxEMW"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=10, \n",
    "    validation_data=(X_test, y_test), \n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "_, train_accuracy = model.evaluate(X_train, y_train)\n",
    "print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "_, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Testing Accuracy: {test_accuracy:.4f}')\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "915IUP131S2O"
   },
   "outputs": [],
   "source": [
    "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "  model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "  model.add(layers.GlobalMaxPooling1D())\n",
    "  model.add(layers.Dense(10, activation='relu'))\n",
    "  model.add(layers.Dense(1, activation='sigmoid'))\n",
    "  model.compile(\n",
    "      optimizer='adam', \n",
    "      loss='binary_crossentropy', \n",
    "      metrics=['accuracy']\n",
    "  )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tx2g28fz3GlM"
   },
   "outputs": [],
   "source": [
    "param_grid = dict(\n",
    "    num_filters=[32, 64, 128],\n",
    "    kernel_size=[3, 5, 7],\n",
    "    vocab_size=[5000],\n",
    "    embedding_dim=[50],\n",
    "    maxlen=[100]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jcNkRB320Ey"
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Main settings\n",
    "epochs = 20\n",
    "embedding_dim = 50\n",
    "maxlen = 100\n",
    "output_file = 'data/output.txt'\n",
    "\n",
    "# Run grid search for each source (yelp, amazon, imdb)\n",
    "for source, frame in df.groupby('source'):\n",
    "    print('Running grid search for data set :', source)\n",
    "    sentences = df['sentence'].values\n",
    "    y = df['label'].values\n",
    "\n",
    "    # Train-test split\n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "        sentences, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "    # Tokenize words\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(sentences_train)\n",
    "    X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "    X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "    # Adding 1 because of reserved 0 index\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Pad sequences with zeros\n",
    "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "    # Parameter grid for grid search\n",
    "    param_grid = dict(num_filters=[32, 64, 128],\n",
    "                      kernel_size=[3, 5, 7],\n",
    "                      vocab_size=[vocab_size],\n",
    "                      embedding_dim=[embedding_dim],\n",
    "                      maxlen=[maxlen])\n",
    "    model = KerasClassifier(build_fn=create_model,\n",
    "                            epochs=epochs, batch_size=10,\n",
    "                            verbose=False)\n",
    "    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
    "                              cv=4, verbose=1, n_iter=5)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate testing set\n",
    "    test_accuracy = grid.score(X_test, y_test)\n",
    "\n",
    "    # Save and evaluate results\n",
    "    prompt = input(f'finished {source}; write to file and proceed? [y/n]')\n",
    "    if prompt.lower() not in {'y', 'true', 'yes'}:\n",
    "        break\n",
    "    with open(output_file, 'a') as f:\n",
    "        s = ('Running {} data set\\nBest Accuracy : '\n",
    "             '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n",
    "        output_string = s.format(\n",
    "            source,\n",
    "            grid_result.best_score_,\n",
    "            grid_result.best_params_,\n",
    "            test_accuracy)\n",
    "        print(output_string)\n",
    "        f.write(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2Pzu8P33nK4"
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RPKerasNLPDemo",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
